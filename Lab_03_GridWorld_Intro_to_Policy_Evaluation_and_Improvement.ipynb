{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bUB4tPSl9Lim"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5wblBXVY9eZy"
      },
      "outputs": [],
      "source": [
        "# States of a gridworld\n",
        "N = 5\n",
        "M = 5\n",
        "\n",
        "# state space\n",
        "state_space = list(itertools.product(range(N), range(M)))\n",
        "\n",
        "# action space\n",
        "action_space = [(0,1), (0,-1), (1,0), (-1,0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kL8f6I54NET",
        "outputId": "5f4c7b19-f519-47ea-9a85-a8a3b30b8678"
      },
      "outputs": [],
      "source": [
        "list(itertools.product(range(N), range(M)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lngjbjb9kUG"
      },
      "source": [
        "### Rules\n",
        "\n",
        "|   | 0   | 1   | 2   | 3   | 4   |\n",
        "|---|-----|-----|-----|-----|-----|\n",
        "| 0 |     |     |     |     |     |\n",
        "| 1 |     |     |     |     |     |\n",
        "| 2 |     |     |     |     |     |\n",
        "| 3 |     |     |     |     |     |\n",
        "| 4 |     |     |     |     | END |\n",
        "\n",
        "--------------\n",
        "\n",
        "Terminal state = (4,4)\n",
        "\n",
        "- Cannot go outside the grid.\n",
        "- Once the teminal state is reached, the episode ends.\n",
        "\n",
        "- Rewards\n",
        "  - Transition to terminal state gives one step reward of +10 and every other transition gives a reward of -1.\n",
        "\n",
        "--------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Bh-6uVbn4yWb"
      },
      "outputs": [],
      "source": [
        "terminal_states = [(4,4)]\n",
        "\n",
        "def transition_probability(start_state, action, end_state):\n",
        "  if start_state in terminal_states:\n",
        "    return 0\n",
        "\n",
        "  expected_state = tuple(np.array(start_state) + np.array(action))\n",
        "  if expected_state == end_state:\n",
        "    return 1\n",
        "\n",
        "  if expected_state not in state_space and start_state == end_state:\n",
        "    return 1\n",
        "\n",
        "  return 0\n",
        "\n",
        "def reward(start_state, action, end_state):\n",
        "  if end_state in terminal_states:\n",
        "    return 10\n",
        "  else:\n",
        "    return -1\n",
        "\n",
        "def random_policy(start_state, action):\n",
        "  if action in [(1,0), (0,1)]:\n",
        "    return 0.5\n",
        "  else:\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK8jk7SE6S3O",
        "outputId": "3adc3053-0540-4674-add7-9e2ed8239ff2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transition_probability((0,0), (-1,0), (0,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z99IKfAALBX"
      },
      "source": [
        "### Policy evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FBoqzuOGASc7"
      },
      "outputs": [],
      "source": [
        "gamma = 1\n",
        "\n",
        "v = dict(zip(state_space, np.zeros(N*M)))\n",
        "\n",
        "iter = 0\n",
        "while iter< 1000:\n",
        "  for s in state_space:\n",
        "    term1 = 0\n",
        "    for a in action_space:\n",
        "      term2 = 0\n",
        "      for s_prime in state_space:\n",
        "        term2+= transition_probability(s, a, s_prime) * (reward(s, a, s_prime) + gamma*v[s_prime])\n",
        "      term1 += random_policy(s, a) * term2\n",
        "    v[s] = term1.round(3)\n",
        "  iter+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw5q6l9IBjhO",
        "outputId": "f0119fed-562f-4e92-a91c-9d16faaebf6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.811, 1.811, 2.498, 2.873, 2.998],\n",
              "       [1.811, 3.124, 4.124, 4.748, 4.998],\n",
              "       [2.498, 4.124, 5.499, 6.499, 6.999],\n",
              "       [2.873, 4.748, 6.499, 8.   , 9.   ],\n",
              "       [2.998, 4.998, 6.999, 9.   , 0.   ]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(list(v.values())).reshape(N,M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cl8Z4gpxFRk1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\narray([[0.811, 1.811, 2.498, 2.873, 2.998],\\n       [1.811, 3.124, 4.124, 4.748, 4.998],\\n       [2.498, 4.124, 5.499, 6.499, 6.999],\\n       [2.873, 4.748, 6.499, 8.   , 9.   ],\\n       [2.998, 4.998, 6.999, 9.   , 0.   ]])\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Random policy\n",
        "\n",
        "# random among 4 actions\n",
        "\n",
        "'''\n",
        "array([[-95.785, -93.786, -90.348, -86.592, -84.048],\n",
        "       [-93.786, -91.227, -86.668, -81.382, -77.505],\n",
        "       [-90.348, -86.668, -79.716, -70.764, -63.085],\n",
        "       [-86.592, -81.382, -70.764, -54.875, -36.986],\n",
        "       [-84.048, -77.505, -63.085, -36.986,   0.   ]])\n",
        "'''\n",
        "\n",
        "# random among 2 actions - down or right\n",
        "\n",
        "'''\n",
        "array([[0.811, 1.811, 2.498, 2.873, 2.998],\n",
        "       [1.811, 3.124, 4.124, 4.748, 4.998],\n",
        "       [2.498, 4.124, 5.499, 6.499, 6.999],\n",
        "       [2.873, 4.748, 6.499, 8.   , 9.   ],\n",
        "       [2.998, 4.998, 6.999, 9.   , 0.   ]])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS9xioc8GxFV"
      },
      "source": [
        "### Policy improvement\n",
        "#### Value iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfdC6EcBG62t",
        "outputId": "5048994b-de1e-4364-b51c-391adb432c95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 3.,  4.,  5.,  6.,  7.],\n",
              "       [ 4.,  5.,  6.,  7.,  8.],\n",
              "       [ 5.,  6.,  7.,  8.,  9.],\n",
              "       [ 6.,  7.,  8.,  9., 10.],\n",
              "       [ 7.,  8.,  9., 10.,  0.]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gamma = 1\n",
        "\n",
        "v = dict(zip(state_space, np.zeros(N*M)))\n",
        "\n",
        "iter = 0\n",
        "while iter< 1000:\n",
        "  for s in state_space:\n",
        "    max = -np.inf\n",
        "    for a in action_space:\n",
        "      term2 = 0\n",
        "      for s_prime in state_space:\n",
        "        term2+= transition_probability(s, a, s_prime) * (reward(s, a, s_prime) + gamma*v[s_prime])\n",
        "      if term2 > max:\n",
        "        max = term2\n",
        "    v[s] = max\n",
        "  iter+=1\n",
        "\n",
        "np.array(list(v.values())).reshape(N,M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aK17BEZkI3nk"
      },
      "outputs": [],
      "source": [
        "#Hi AIML2023, Here is a task for the next lab session. \n",
        "#Extend the notebook discussed today to come up with optimal state values through value iteration method for the follwing cases:\n",
        "# Case 1. States (1,2), (1,3) are dummy states. Meaning the agent cannot transition to these states. Modify the transition probability function and/or reward funcion as \n",
        "#required and come up with optimal state values by policy improvemet (value iteration) for each of the above cases.\n",
        "\n",
        "def transition_probability1(start_state, action, end_state):\n",
        "    if start_state in terminal_states or start_state in [(1,2), (1,3)]:\n",
        "        return 0\n",
        "\n",
        "    expected_state = tuple(np.array(start_state) + np.array(action))\n",
        "    if expected_state == end_state:\n",
        "        return 1\n",
        "\n",
        "    if expected_state not in state_space and start_state == end_state:\n",
        "        return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "def reward(start_state, action, end_state):\n",
        "    if end_state in terminal_states:\n",
        "        return 10\n",
        "    elif start_state in [(1,2), (1,3)]:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "terminal_states = [(4,4)]\n",
        "transition_probability1((0,0), (0,-1), (0,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Case 2. States (1,2), (1,3) are damping states, they slow down the agent if it reaches these states. \n",
        "#Assign a one-step-reward of \"-5\" for transition from damping states.\n",
        "\n",
        "def transition_probability2(start_state, action, end_state):\n",
        "    if start_state in terminal_states or start_state in [(1,2), (1,3)]:\n",
        "        return 0\n",
        "\n",
        "    expected_state = tuple(np.array(start_state) + np.array(action))\n",
        "    if expected_state == end_state:\n",
        "        return 1\n",
        "\n",
        "    if expected_state not in state_space and start_state == end_state:\n",
        "        return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "def reward(start_state, action, end_state):\n",
        "    if end_state in terminal_states:\n",
        "        return 10\n",
        "    elif start_state in [(1,2), (1,3)]:\n",
        "        return -5\n",
        "    else:\n",
        "        return -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "terminal_states = [(5,4)]\n",
        "transition_probability2((0,0), (0,-1), (0,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Case 3. States (1,2), (1, 3) are holes. \n",
        "#Meaning the agent cannot come out of these states and episode ends if the agent steps into holes.\n",
        "def transition_probability3(start_state, action, end_state):\n",
        "    if start_state in terminal_states or start_state in [(1,2), (1,3)]:\n",
        "        return 0\n",
        "\n",
        "    expected_state = tuple(np.array(start_state) + np.array(action))\n",
        "    if expected_state == end_state:\n",
        "        return 1\n",
        "\n",
        "    if expected_state not in state_space and start_state == end_state:\n",
        "        return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "def reward(start_state, action, end_state):\n",
        "    if end_state in terminal_states:\n",
        "        return 10\n",
        "    elif start_state in [(1,2), (1,3)]:\n",
        "        return -10  # Assign a negative reward for stepping into holes\n",
        "    else:\n",
        "        return -1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "terminal_states = [(4,4)]\n",
        "transition_probability3((0,0), (0,-1), (0,0))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
